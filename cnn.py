# -*- coding: utf-8 -*-
"""CNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1i7weqUejtwRyGR-fU7IKTelpurZi2N5h
"""

import torch
import matplotlib.pyplot as plt
import numpy as np

import torchvision
import torchvision.transforms as transforms
trainset = torchvision.datasets.CIFAR10(root='./data', train=True,               # train set 
                                        download=True, 
                                        transform=transforms.ToTensor())

classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')

trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True)     # we are giving 4 images batch and shuffle every time

dataiter = iter(trainloader)                                 # train loader given to dataiter so that it will iter every time
images, labels = dataiter.next()

print(images.shape)                      # 4 images 3 rgb 32x32 dimensions

print(images[1].shape)                      # only 2nd inage in batch size of 4 images 3 rgb,32x32 
print(labels[1].item())                 #  in class what is that position of  2nd image in batch of 4 images

img = images[1]
print(type(img))

npimg = img.numpy()
print(npimg.shape)

npimg = np.transpose(npimg, (1, 2, 0))     # repositioning dimensions
print(npimg.shape)

plt.figure(figsize = (1,1))
plt.imshow(npimg)
plt.show()

def imshow(img):
    npimg = img.numpy()
    plt.imshow(np.transpose(npimg, (1, 2, 0)))
    plt.show()

imshow(torchvision.utils.make_grid(images))        # making grid of images
print(' '.join(classes[labels[j]] for j in range(4)))    #label names joining

import torch.nn as nn

class FirstCNN(nn.Module):
    def __init__(self): 
        super(FirstCNN, self).__init__()
        self.conv1 = nn.Conv2d(3, 16, 3, padding=(1,1), stride=(2,2)) # padding=(1,1), stride=(2,2))
        
    def forward(self, x):
        x = self.conv1(x)
        return x

net = FirstCNN()

out = net(images)
out.shape
out

for param in net.parameters():       # it will show parameters (net.parameters())
    print(param.shape)                        # filter details 16 filters,3x3 filter depth 3

out1 = out[0, 0, :, :].detach().numpy()        # in out (torch.Size([4, 16, 16, 16]))     0 means 1 st image ,0 means in that 16 filters 1st filter ,  :,: is 16x16 means we are trying to see  only 1 filter applied to 1 image
print(out1.shape)

plt.imshow(out[0, 0, :, :].detach().numpy())
plt.show()

class FirstCNN_v2(nn.Module):
    def __init__(self): 
        super(FirstCNN_v2, self).__init__()
        self.model = nn.Sequential(
            nn.Conv2d(3, 8, 3),   # (N, 3, 32, 32) -> (N, 8, 30, 30)           # 8 filters(outputs) become inputs later layer
            nn.Conv2d(8, 16, 3)   # (N, 8, 30, 30) -> (N, 16, 28, 28)
        )
        
    def forward(self, x):
        x = self.model(x)
        return x

net = FirstCNN_v2()
out = net(images)
out.shape

plt.imshow(out[0, 0, :, :].detach().numpy())

class FirstCNN_v3(nn.Module):
    def __init__(self): 
        super(FirstCNN_v3, self).__init__()
        self.model = nn.Sequential(
            nn.Conv2d(3, 6, 5),          # (N, 3, 32, 32) -> (N, 6, 28, 28)    # 3 is depth ,rgb and 6 is filters and filter size is 3x3
            nn.AvgPool2d(2, stride=2),   # (N, 6, 28, 28) -> (N, 6, 14, 14)
            nn.Conv2d(6, 16, 5),         # (N, 6, 14, 14) -> (N, 16, 10, 10)
            nn.AvgPool2d(2, stride=2)    # (N, 16, 10, 10) -> (N, 16, 5, 5)
        )
        
    def forward(self, x):
        x = self.model(x)
        return x

net = FirstCNN_v3()
out = net(images)
out.shape

plt.imshow(out[0, 0, :, :].detach().numpy())

class LeNet(nn.Module):
    def __init__(self): 
        super(LeNet, self).__init__()
        self.cnn_model = nn.Sequential(
            nn.Conv2d(3, 6, 5),         # (N, 3, 32, 32) -> (N,  6, 28, 28)
            nn.Tanh(),
            nn.AvgPool2d(2, stride=2),  # (N, 6, 28, 28) -> (N,  6, 14, 14)
            nn.Conv2d(6, 16, 5),        # (N, 6, 14, 14) -> (N, 16, 10, 10)  
            nn.Tanh(),
            nn.AvgPool2d(2, stride=2)   # (N,16, 10, 10) -> (N, 16, 5, 5)
        )
        self.fc_model = nn.Sequential(
            nn.Linear(400,120),         # (N, 400) -> (N, 120)                          # cnn out put become here input 16x5x5 =400
            nn.Tanh(),
            nn.Linear(120,84),          # (N, 120) -> (N, 84)
            nn.Tanh(),
            nn.Linear(84,10)            # (N, 84)  -> (N, 10)
        )
        
    def forward(self, x):
        print(x.shape)
        x = self.cnn_model(x)                  # first we are giving inputs to cnn
        print(x.shape)                                #torch.Size([4, 16, 5, 5])
        x = x.view(x.size(0), -1)              #  0 is 4 ,-1 is 16x5x5  it will multiply remaining
        print(x.shape)
        x = self.fc_model(x)              # then giving that  to FNN
        print(x.shape)
        return x

net = LeNet()
out = net(images)

print(out)

max_values, pred_class = torch.max(out.data, 1)                   # maximum is output
print(pred_class)                                                                            # indexes we got (1st image is 6(frog))..etc

class LeNet(nn.Module):
    def __init__(self): 
        super(LeNet, self).__init__()
        self.cnn_model = nn.Sequential(
            nn.Conv2d(3, 6, 5),         # (N, 3, 32, 32) -> (N,  6, 28, 28)
            nn.Tanh(),
            nn.AvgPool2d(2, stride=2),  # (N, 6, 28, 28) -> (N,  6, 14, 14)
            nn.Conv2d(6, 16, 5),        # (N, 6, 14, 14) -> (N, 16, 10, 10)  
            nn.Tanh(),
            nn.AvgPool2d(2, stride=2)   # (N,16, 10, 10) -> (N, 16, 5, 5)
        )
        self.fc_model = nn.Sequential(
            nn.Linear(400,120),         # (N, 400) -> (N, 120)
            nn.Tanh(),
            nn.Linear(120,84),          # (N, 120) -> (N, 84)
            nn.Tanh(),
            nn.Linear(84,10)            # (N, 84)  -> (N, 10)
        )
        
    def forward(self, x):
        x = self.cnn_model(x)
        x = x.view(x.size(0), -1)
        x = self.fc_model(x)
        return x

batch_size = 128
trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms.ToTensor())
trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)
testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transforms.ToTensor())
testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False)

def evaluation(dataloader):
    total, correct = 0, 0
    for data in dataloader:
        inputs, labels = data
        outputs = net(inputs)
        _, pred = torch.max(outputs.data, 1)      # pred is label 
        total += labels.size(0)
        correct += (pred == labels).sum().item()
    return 100 * correct / total



net = LeNet()

import torch.optim as optim

loss_fn = nn.CrossEntropyLoss()
opt = optim.Adam(net.parameters())

# Commented out IPython magic to ensure Python compatibility.
# %%time
# loss_arr = []
# loss_epoch_arr = []
# max_epochs = 16
# 
# for epoch in range(max_epochs):
# 
#     for i, data in enumerate(trainloader, 0):
# 
#         inputs, labels = data
# 
#         opt.zero_grad()
# 
#         outputs = net(inputs)
#         loss = loss_fn(outputs, labels)
#         loss.backward()
#         opt.step()
#         
#         loss_arr.append(loss.item())
#         
#     loss_epoch_arr.append(loss.item())
#         
#     print('Epoch: %d/%d, Test acc: %0.2f, Train acc: %0.2f' % (epoch, max_epochs, evaluation(testloader), evaluation(trainloader)))
#     
#     
# plt.plot(loss_epoch_arr)
# plt.show()

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print(device)

def evaluation(dataloader):
    total, correct = 0, 0
    for data in dataloader:
        inputs, labels = data
        inputs, labels = inputs.to(device), labels.to(device)
        outputs = net(inputs)
        _, pred = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (pred == labels).sum().item()
    return 100 * correct / total

net = LeNet().to(device)
loss_fn = nn.CrossEntropyLoss()
opt = optim.Adam(net.parameters())

# Commented out IPython magic to ensure Python compatibility.
# %%time
# max_epochs = 16
# 
# for epoch in range(max_epochs):
# 
#     for i, data in enumerate(trainloader, 0):
# 
#         inputs, labels = data
#         inputs, labels = inputs.to(device), labels.to(device)
# 
#         opt.zero_grad()
# 
#         outputs = net(inputs)
#         loss = loss_fn(outputs, labels)
#         loss.backward()
#         opt.step()
#         
#     print('Epoch: %d/%d' % (epoch, max_epochs))

print('Test acc: %0.2f, Train acc: %0.2f' % (evaluation(testloader), evaluation(trainloader)))

imshow(torchvision.utils.make_grid(images))

net = net.to('cpu')

out = net(images)
print(out.shape)

out = net.cnn_model[0](images)
out.shape

image_id = 3
plt.figure(figsize = (2,2))
imshow(images[image_id,])

plt.figure(figsize = (6,6))
plt.subplot(321)
for i in range(6):
    ax1 = plt.subplot(3, 2, i+1)
    plt.imshow(out[image_id, i, :, :].detach().numpy(), cmap="binary")
plt.show()



